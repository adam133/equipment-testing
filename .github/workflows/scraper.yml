name: Equipment Scraper

on:
  # Run weekly on Sundays at 2 AM UTC
  schedule:
    - cron: '0 2 * * 0'
  # Allow manual triggering
  workflow_dispatch:
    inputs:
      spider_name:
        description: 'Spider to run (leave empty for all)'
        required: false
        default: ''

jobs:
  scrape:
    name: Run Equipment Scrapers
    runs-on: ubuntu-latest
    environment: production

    steps:
    - uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Set up Python
      run: uv python install 3.12

    - name: Install dependencies
      run: |
        uv sync --all-extras

    - name: Run scrapers
      env:
        # Unity Catalog credentials for DuckDB integration
        DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
        DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      run: |
        if [ -z "${{ github.event.inputs.spider_name }}" ]; then
          echo "Running all functional spiders..."
          
          # Get list of all spiders and store in temporary file for safe iteration
          uv run scrapy list > /tmp/spider_list.txt
          
          # Check each spider and run only those with start_urls configured
          while IFS= read -r spider; do
            echo "Checking spider: $spider"
            
            # Check if spider has start_urls using a Python script
            # Pass spider name as stdin to avoid injection issues
            has_urls=$(echo "$spider" | uv run python -c "
import sys
from scrapy.utils.project import get_project_settings
from scrapy.spiderloader import SpiderLoader

spider_name = sys.stdin.read().strip()
settings = get_project_settings()
loader = SpiderLoader.from_settings(settings)

try:
    spider_cls = loader.load(spider_name)
    # Check if spider has start_urls
    if hasattr(spider_cls, 'start_urls') and spider_cls.start_urls:
        print('yes')
    else:
        print('no')
except Exception:
    print('no')
" 2>/dev/null || echo "no")
            
            if [ "$has_urls" = "yes" ]; then
              echo "Running spider: $spider"
              uv run scrapy crawl "$spider" -o "${spider}_output.json" || echo "Spider $spider failed, continuing..."
            else
              echo "Skipping spider $spider (no start_urls configured)"
            fi
          done < /tmp/spider_list.txt
        else
          # Validate spider name exists before running
          spider_name="${{ github.event.inputs.spider_name }}"
          echo "Validating spider: $spider_name"
          
          # Get list of all available spiders
          uv run scrapy list > /tmp/spider_list.txt
          
          # Check if provided spider name is in the list
          spider_found=false
          while IFS= read -r available_spider; do
            if [ "$available_spider" = "$spider_name" ]; then
              spider_found=true
              break
            fi
          done < /tmp/spider_list.txt
          
          if [ "$spider_found" = "true" ]; then
            echo "Running spider: $spider_name"
            uv run scrapy crawl "$spider_name" -o "${spider_name}_output.json"
          else
            echo "Error: Spider '$spider_name' not found"
            echo "Available spiders:"
            cat /tmp/spider_list.txt
            exit 1
          fi
        fi

    - name: Upload scraper logs and output
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-logs
        path: |
          *.log
          *_output.json
          scrapy_*.json
        retention-days: 7
