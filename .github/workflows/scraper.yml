name: Equipment Scraper

on:
  # Run weekly on Sundays at 2 AM UTC
  schedule:
    - cron: '0 2 * * 0'
  # Allow manual triggering
  workflow_dispatch:
    inputs:
      spider_name:
        description: 'Spider to run (leave empty for all)'
        required: false
        default: ''

jobs:
  scrape:
    name: Run Equipment Scrapers
    runs-on: ubuntu-latest
    environment: production

    steps:
    - uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Set up Python
      run: uv python install 3.12

    - name: Install dependencies
      run: |
        uv sync --all-extras

    - name: Run scrapers
      env:
        # Unity Catalog credentials for DuckDB integration
        DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
        DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      run: |
        if [ -z "${{ github.event.inputs.spider_name }}" ]; then
          echo "Running all spiders with start_urls configured..."
          
          # List all available spiders
          echo "Available spiders:"
          uv run scrapy list
          echo
          
          # Define spiders to run (spiders with start_urls)
          # Note: We explicitly list functional spiders to avoid running base/template spiders
          spiders_to_run="quality_farm_supply"
          
          # Track failures
          failed_spiders=""
          
          # Run each spider
          for spider in $spiders_to_run; do
            echo "Running spider: $spider"
            if ! uv run scrapy crawl "$spider" -o "${spider}_output.json"; then
              echo "ERROR: Spider $spider failed"
              failed_spiders="${failed_spiders} ${spider}"
            else
              echo "SUCCESS: Spider $spider completed"
            fi
            echo
          done
          
          # Report any failures
          if [ -n "$failed_spiders" ]; then
            echo "=========================================="
            echo "WARNING: Some spiders failed:${failed_spiders}"
            echo "=========================================="
            exit 1
          else
            echo "All spiders completed successfully"
          fi
        else
          # Run specific spider
          spider_name="${{ github.event.inputs.spider_name }}"
          echo "Running spider: $spider_name"
          
          # Verify spider exists
          if ! uv run scrapy list | grep -q "^${spider_name}$"; then
            echo "Error: Spider '$spider_name' not found"
            echo "Available spiders:"
            uv run scrapy list
            exit 1
          fi
          
          # Run the spider
          uv run scrapy crawl "$spider_name" -o "${spider_name}_output.json"
        fi

    - name: Upload scraper logs and output
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-logs
        path: |
          *.log
          *_output.json
          scrapy_*.json
        retention-days: 7
